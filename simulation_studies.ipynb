{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation studies in Python\n",
    "\n",
    "This notebook illustrates techniques for conducting simulation studies in Python.\n",
    "The main focus here is to better understand that properties of statistical\n",
    "methods in various settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "\n",
    "nrep = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation for assessing the standard error\n",
    "\n",
    "One of the most basic facts in statistics is that the sample mean of independent\n",
    "and identically distributed (iid) data has the form $\\sigma/\\sqrt{n}$, where\n",
    "$n$ is the sample size, and $\\sigma$ is the standard deviation.  We don't need\n",
    "a simulation study to verify this provable fact, but we can use this as an\n",
    "opportunity to conduct a simulation study in a setting where we know what the\n",
    "answer is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample sizes\n",
    "for nobs in 50, 200, 800:\n",
    "\n",
    "    # Standard deviation\n",
    "    for sig in 0.5, 1, 2:\n",
    "\n",
    "        # Population distributions\n",
    "        for dist in 0, 1:\n",
    "\n",
    "            # Simulate the data\n",
    "            if dist == 0:\n",
    "                x = sig * np.random.normal(size=(nrep, nobs))\n",
    "            elif dist == 1:\n",
    "                df = 4\n",
    "                f = np.sqrt((df - 2) / df)\n",
    "                x = sig * f * np.random.standard_t(size=(nrep, nobs), df=df)\n",
    "\n",
    "            # The standard error (obtained from the simulation)\n",
    "            se = x.mean(1).std()\n",
    "\n",
    "            # The standard error based on theory\n",
    "            se_theory = sig / np.sqrt(nobs)\n",
    "\n",
    "            print(\"%5d %12.4f %8.0f %12.4f %12.4f\" % (nobs, sig, dist, se, se_theory))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A slightly more realistic example is to look at the standard error of the Pearson\n",
    "correlation coefficient.  Unlike in the case of the sample mean for IID data, here we only\n",
    "have an approximate analytic formula that the standard error is roughly $1/\\sqrt{n}$.\n",
    "The following simulation shows that when the population correlation is zero, this formula\n",
    "works well when the true correlation is\n",
    "small, for both normal and heavy-tailed data distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sample sizes\n",
    "for nobs in 10, 50, 100:\n",
    "\n",
    "    # The population distributions\n",
    "    for dist in 0, 1:\n",
    "\n",
    "        # Generate the data\n",
    "        if dist == 0:\n",
    "            x1 = sig * np.random.normal(size=(nrep, nobs))\n",
    "            x2 = sig * np.random.normal(size=(nrep, nobs))\n",
    "        elif dist == 1:\n",
    "            df = 4\n",
    "            f = np.sqrt((df - 2) / df)\n",
    "            x1 = sig * f * np.random.standard_t(size=(nrep, nobs), df=df)\n",
    "            x2 = sig * f * np.random.standard_t(size=(nrep, nobs), df=df)\n",
    "\n",
    "        # The sample correlation coefficients\n",
    "        cor = [np.corrcoef(x1[i, :], x2[i, :])[0, 1] for i in range(nrep)]\n",
    "        cor = np.asarray(cor)\n",
    "\n",
    "        # The standard error obtained via simulation\n",
    "        se = cor.std()\n",
    "\n",
    "        # The standard error based on theory\n",
    "        se_theory = 1 / np.sqrt(nobs)\n",
    "\n",
    "        print(\"%5d %8.0f %12.4f %12.4f\" % (nobs, dist, se, se_theory))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we turn to settings where the true correlation is positive.  The Pearson\n",
    "correlation coefficient has an inverse \"mean/variance relationship\" --\n",
    "when the correlation is large, the standard deviation is small, and vice versa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample sizes\n",
    "for nobs in 10, 50, 100:\n",
    "\n",
    "    # The population distributin\n",
    "    for dist in 0, 1:\n",
    "\n",
    "        # The population correlation\n",
    "        for r in 0.25, 0.5, 0.75:\n",
    "\n",
    "            # Generate the data\n",
    "            if dist == 0:\n",
    "                x1 = sig * np.random.normal(size=(nrep, nobs))\n",
    "                x2 = sig * np.random.normal(size=(nrep, nobs))\n",
    "            elif dist == 1:\n",
    "                df = 4\n",
    "                f = np.sqrt((df - 2) / df)\n",
    "                x1 = sig * f * np.random.standard_t(size=(nrep, nobs), df=df)\n",
    "                x2 = sig * f * np.random.standard_t(size=(nrep, nobs), df=df)\n",
    "\n",
    "            # Induce the correlation\n",
    "            x2 = r*x1 + np.sqrt(1 - r**2)*x2\n",
    "\n",
    "            # Calculate the sample correlations\n",
    "            cor = [np.corrcoef(x1[i, :], x2[i, :])[0, 1] for i in range(nrep)]\n",
    "            cor = np.asarray(cor)\n",
    "\n",
    "            # The standard error based on the simulation\n",
    "            se = cor.std()\n",
    "\n",
    "            # The (approxiate) standard deviation based on theory\n",
    "            se_theory = 1 / np.sqrt(nobs)\n",
    "\n",
    "            print(\"%5d %8.0f %12.4f %12.4f %12.4f\" % (nobs, dist, r, se, se_theory))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A \"variance-stabilizing transformation\" is a transformation of the data that removes\n",
    "a mean-variance relationship.  For normal data, the \"Fisher Z-transform\" (arc-tangent\n",
    "function) is an approximate variance stabilizing transformation, but it is not\n",
    "very effective for heavier-tailed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample sizes\n",
    "for nobs in 10, 50, 100:\n",
    "\n",
    "    # Population distribution (normal or t)\n",
    "    for dist in 0, 1:\n",
    "\n",
    "        # Population correlation\n",
    "        for r in 0.25, 0.5, 0.75:\n",
    "\n",
    "            if dist == 0:\n",
    "                x1 = sig * np.random.normal(size=(nrep, nobs))\n",
    "                x2 = sig * np.random.normal(size=(nrep, nobs))\n",
    "            elif dist == 1:\n",
    "                df = 4\n",
    "                f = np.sqrt((df - 2) / df)\n",
    "                x1 = sig * f * np.random.standard_t(size=(nrep, nobs), df=df)\n",
    "                x2 = sig * f * np.random.standard_t(size=(nrep, nobs), df=df)\n",
    "\n",
    "            # Induce the correlation\n",
    "            x2 = r*x1 + np.sqrt(1 - r**2)*x2\n",
    "\n",
    "            # Get the sample correlation\n",
    "            cor = [np.corrcoef(x1[i, :], x2[i, :])[0, 1] for i in range(nrep)]\n",
    "            cor = np.asarray(cor)\n",
    "\n",
    "            # Apply the Fisher transformation\n",
    "            tcor = 0.5 * np.log((1 + cor) / (1 - cor))\n",
    "\n",
    "            # The simulation-based standard error\n",
    "            se = tcor.std()\n",
    "\n",
    "            # The theoretical SE\n",
    "            se_theory = 1 / np.sqrt(nobs - 3)\n",
    "\n",
    "            print(\"%5d %8.0f %12.4f %12.4f %12.4f\" % (nobs, dist, r, se, se_theory))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final example of using simulation to assess standard errors, we can consider\n",
    "$2 \\times 2$ contingency tables.  Here we focus\n",
    "on the log odds ratio, which is a commonly-used statistic for assessing the degree\n",
    "of association between the rows and the columns of the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The expected total sample size\n",
    "for n in 50, 100:\n",
    "\n",
    "    # The population odds ratio between rows/columns\n",
    "    for oddsratio in 1, 2:\n",
    "\n",
    "        # Marginal probabilities of the first row\n",
    "        for p1 in 0.1, 0.5:\n",
    "\n",
    "            # Marginal probabilities of the second row\n",
    "            for p2 in 0.1, 0.5:\n",
    "\n",
    "                # Get the cell probabilities\n",
    "                # https://en.wikipedia.org/wiki/Odds_ratio\n",
    "                if oddsratio != 1:\n",
    "                    s = np.sqrt((1 + (p1 + p2)*(oddsratio - 1))**2 + 4*oddsratio*(1-oddsratio)*p1*p2)\n",
    "                    p11 = (1 + (p1 + p2) * (oddsratio - 1) - s) / (2 * (oddsratio - 1))\n",
    "                else:\n",
    "                    p11 = p1 * p2\n",
    "                p12 = p1 - p11\n",
    "                p21 = p2 - p11\n",
    "                p22 = 1 - (p11 + p12 + p21)\n",
    "\n",
    "                # Simulate the data for nrep 2x2 tables\n",
    "                table = np.zeros((nrep, 4))\n",
    "                table[:, 0] = np.random.poisson(n*p11, size=nrep)\n",
    "                table[:, 1] = np.random.poisson(n*p12, size=nrep)\n",
    "                table[:, 2] = np.random.poisson(n*p21, size=nrep)\n",
    "                table[:, 3] = np.random.poisson(n*p22, size=nrep)\n",
    "\n",
    "                # Get the sample log odds ratio (LOR) for every table\n",
    "                sample_lor = np.log(table[:, 0]) + np.log(table[:, 3])\n",
    "                sample_lor -= np.log(table[:, 1]) + np.log(table[:, 2])\n",
    "\n",
    "                # Some LOR values are infinite due to zero cells, we drop\n",
    "                # them here\n",
    "                sample_lor = sample_lor[np.isfinite(sample_lor)]\n",
    "\n",
    "                # The standard error of the LOR based on the simulation\n",
    "                se = sample_lor.std()\n",
    "\n",
    "                # The theoretical SE of the LOR\n",
    "                se_theory = np.sqrt(1/p11 + 1/p12 + 1/p21 + 1/p22) / np.sqrt(n)\n",
    "\n",
    "                print(\"%8d %12.4f %12.4f %12.4f %12.4f %12.4f\" %\n",
    "                      (n, p1, p2, oddsratio, se, se_theory))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample size and power assessment\n",
    "\n",
    "Simulation studies are commonly used for power analysis.  This is a big topic,\n",
    "but broadly speaking, the goal of a power analysis is to assess what effect\n",
    "sizes can be detected when applying a given dataset to a sample from a given\n",
    "population.\n",
    "\n",
    "Some statistical methods are amenable to analytic-based power analyses.  But in many\n",
    "cases it is more practical to use simulation.  Below is an example of a simulation-based\n",
    "power analysis for logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The sample size\n",
    "for n in 100, 200:\n",
    "\n",
    "    # The correlation between two covariates (multicollinearity)\n",
    "    for r in 0, 0.5:\n",
    "\n",
    "        # The effect size\n",
    "        for b in 0.1, 0.5:\n",
    "\n",
    "            power = 0\n",
    "            for k in range(100):\n",
    "\n",
    "                # Generate covariates\n",
    "                x = np.random.normal(size=(n, 3))\n",
    "                x[:, 0] = 1\n",
    "                x[:, 2] = r*x[:, 1] + np.sqrt(1 - r**2)*x[:, 2]\n",
    "\n",
    "                # The linear predictor\n",
    "                lpr = b * x[:, 1]\n",
    "\n",
    "                # The outcome probabilities\n",
    "                pr = 1 / (1 + np.exp(-lpr))\n",
    "\n",
    "                # The outcomes\n",
    "                y = (np.random.uniform(size=n) < pr).astype(np.int)\n",
    "\n",
    "                # Fit the model, and see if we detect the effect of interest\n",
    "                model = sm.GLM(y, x, family=sm.families.Binomial())\n",
    "                result = model.fit()\n",
    "                power += np.abs(result.tvalues[1]) > 2\n",
    "\n",
    "            power /= 100\n",
    "            print(\"%8d %12.4f %12.4f %12.4f\" % (n, r, b, power))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulation studies can also be useful for assessing bias in non-standard\n",
    "settings.  For example, below we consider omitted variable bias in logistic\n",
    "regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "b = 0.5\n",
    "\n",
    "# The sample size\n",
    "for n in 100, 200:\n",
    "\n",
    "    # The correlation between two covariates (multicollinearity)\n",
    "    for r in 0, 0.5, 0.8:\n",
    "\n",
    "        est = 0\n",
    "        for k in range(500):\n",
    "\n",
    "            # Generate covariates\n",
    "            x = np.random.normal(size=(n, 3))\n",
    "            x[:, 0] = 1\n",
    "            x[:, 2] = r*x[:, 1] + np.sqrt(1 - r**2)*x[:, 2]\n",
    "\n",
    "            # The linear predictor\n",
    "            lpr = b * (x[:, 1] + x[:, 2])\n",
    "\n",
    "            # The outcome probabilities\n",
    "            pr = 1 / (1 + np.exp(-lpr))\n",
    "\n",
    "            # The outcomes\n",
    "            y = (np.random.uniform(size=n) < pr).astype(np.int)\n",
    "\n",
    "            # Fit the model omitting one variable\n",
    "            model = sm.GLM(y, x[:, 0:2], family=sm.families.Binomial())\n",
    "            result = model.fit()\n",
    "            est += result.params[1]\n",
    "\n",
    "        est /= 500\n",
    "        print(\"%8d %12.4f %12.4f %12.4f\" % (n, r, b, est))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is another example of a simulation study used to assess bias.  Here\n",
    "the bias is a result of \"informative\" missing data, or selection bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "b = 0.5\n",
    "\n",
    "# The sample size\n",
    "for n in 100, 200:\n",
    "\n",
    "    # The correlation between two covariates (multicollinearity)\n",
    "    for r in 0, 0.5, 0.8:\n",
    "\n",
    "        est = 0\n",
    "        for k in range(500):\n",
    "\n",
    "            # Generate covariates\n",
    "            x = np.random.normal(size=(n, 3))\n",
    "            x[:, 0] = 1\n",
    "            x[:, 2] = r*x[:, 1] + np.sqrt(1 - r**2)*x[:, 2]\n",
    "\n",
    "            # The linear predictor\n",
    "            lpr = b * (x[:, 1] + x[:, 2])\n",
    "\n",
    "            # The outcome probabilities\n",
    "            pr = 1 / (1 + np.exp(-lpr))\n",
    "\n",
    "            # The outcomes\n",
    "            y = (np.random.uniform(size=n) < pr).astype(np.int)\n",
    "\n",
    "            ii = np.flatnonzero(x[:,1] * (2*y-1) + 2*np.random.normal(size=n) > 0)\n",
    "            y = y[ii]\n",
    "            x = x[ii, :]\n",
    "\n",
    "            # Fit the model omitting one variable\n",
    "            model = sm.GLM(y, x[:, 0:2], family=sm.families.Binomial())\n",
    "            result = model.fit()\n",
    "            est += result.params[1]\n",
    "\n",
    "        est /= 500\n",
    "        print(\"%8d %12.4f %12.4f %12.4f\" % (n, r, b, est))"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 1
}
