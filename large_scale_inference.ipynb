{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large scale inference and multiple testing\n",
    "\n",
    "Basic approaches to statistical inference focus on the setting where a\n",
    "researcher wishes to assess a single hypothesis by attempting to\n",
    "falsify it. This has come to be known as \"null hypothesis significance\n",
    "testing\", or the \"Neyman-Pearson\" approach to statistical inference.\n",
    "\n",
    "A properly-conducted statistical hypothesis test controls the\n",
    "\"family-wise\" or \"type-I\" error rate, also known as the \"false\n",
    "positive rate\", which is the probability of rejecting the null\n",
    "hypothesis when the null hypothesis is true.  Typically, the\n",
    "family-wise error rate (FWER) is controlled at 5%, but this is an\n",
    "arbitrary threshold.\n",
    "\n",
    "In practice, it is rarely the case that an empirical research project\n",
    "strictly follows the framework for which null hypothesis significance\n",
    "testing (NHST) was devised.  Research often involves assessing several\n",
    "hypotheses with the same data, or may involve assessing hypotheses\n",
    "that are suggested through preliminary analysis of the data.  Famous\n",
    "approaches by Sidak, Scheffe, Bonferroni, and Tukey have been\n",
    "developed to address this issue, and are effective in many settings.\n",
    "\n",
    "More recently, alternative frameworks for statistical inference that\n",
    "are suited for a broader set of research strategies have been devised.\n",
    "One important notion is that of a \"false discovery rate\", which we\n",
    "will discuss further below.  A branch of statistics often called\n",
    "\"large scale inference\" has arisen to address statistical inference\n",
    "for complex data analysis workflows.  We will illustrate a few basic\n",
    "techniques in this area that can be carried out using the Python\n",
    "statsmodels library, or by direct calculation using the Numpy Python\n",
    "module.\n",
    "\n",
    "# p-values and multiple testing\n",
    "\n",
    "In many scientific investigations, the most interesting outcome that a\n",
    "researcher can hope for is to \"reject\" a simple null hypothesis.  This\n",
    "typically allows the researcher to claim that they have discovered a\n",
    "new association, predictive relationship, or mechanism.  In other\n",
    "words, classical NHST analyses are usually conducted in a setting\n",
    "where the researcher wants to see the null hypothesis rejected.  To\n",
    "counteract this, a primary goal of traditional (NHST) inference\n",
    "procedures is to control the probability of incorrectly rejecting the\n",
    "null hypothesis.  By convention, this probability is often bounded at\n",
    "5% (the family-wise error rate, or FWER).\n",
    "\n",
    "Since the FWER cannot in practice be controlled at zero, we allow the\n",
    "null hypothesis to be falsely rejected with a certain positive\n",
    "probability (e.g. 5%).  This is the probability that a false assertion\n",
    "is made based on the analysis.  While a 5% rate of false conclusions\n",
    "may be acceptable, the rate of false assertions grows as the\n",
    "researcher conducts additional analyses related to the same research\n",
    "aim.  Essentially, this gives the researcher multiple chances to\n",
    "\"win\".  This is the fundamental issue that arises when applying the\n",
    "NHST framework to more open-ended research pipelines.\n",
    "\n",
    "To make this more concrete, below are several specific settings where\n",
    "multiple testing can easily arise:\n",
    "\n",
    "* _Subgroup analyses_: Suppose we have a clinical trial in which the\n",
    "goal is to assess the effectiveness of a treatment.  The null\n",
    "hypothesis is that the treatment has no effect, and the alternative\n",
    "hypothesis is that the treatment is beneficial.  At the outset of such\n",
    "a study, the researcher usually aims to show that the treatment is\n",
    "beneficial in the population represented by the subjects in the\n",
    "clinical trial.  But it is common to ask in addition if the treatment\n",
    "is beneficial in a subpopulation, e.g. only in women, only in older\n",
    "people, or only in people with a certain form of the disease.  If the\n",
    "p-value for at least one of these tests is less than the conventional\n",
    "0.05 threshold, the true \"significance\" of our discovery is unclear.\n",
    "For example, if the p-value is less than 0.05 in the female sample,\n",
    "but not in the whole sample or in any other supopulation, then people\n",
    "may be tempted to interpret this as equivalent to if we had\n",
    "pre-specified that the only question of interest was whether the\n",
    "treatment is effective for women.  However this is misleading.  If the\n",
    "treatment is in fact totally ineffective for all people, and we assess\n",
    "it in five subpopulations, then the FWER can be as high as 20%, even\n",
    "if each test on its own has a false positive rate of 5%.\n",
    "\n",
    "* _Model selection_: Suppose we are conducting an observational study\n",
    "to assess the association between a primary variable of interest and\n",
    "an outcome.  For example, we may be interested in assessing whether\n",
    "states with a particular regulation have slower economic growth than\n",
    "states without the regulation.  Since such regulations are not\n",
    "assigned at random, we cannot simply compare the economic growth rates\n",
    "in states with and without the regulation.  Instead, we typically\n",
    "identify several potential confounding factors and use some form of\n",
    "regression analysis to assess whether the regulation is associated\n",
    "with economic growth when comparing states that are otherwise similar.\n",
    "In general, we will not know what the confounders are, so in practice\n",
    "we will identify many potential confounders, screen out those that\n",
    "seem to play no role, and assess the effect of interest in a model\n",
    "that includes the confounders that seem to be important.  This process\n",
    "of model selection is somewhat open-ended and may lead to a form of\n",
    "implicit multiple testing.  The effect of interest will be assessed\n",
    "many times, adjusting for different sets of potential confounding\n",
    "variables.  Each model may control the false positive rate for the\n",
    "effect of interest at 5%, but over all such models, the FWER will\n",
    "exceed 5%, possibly by a large margin.\n",
    "\n",
    "To begin, we will conduct some simple simulations to show how FWER\n",
    "inflation can occur.  As always, we first import the Python libraries\n",
    "that we will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Remove these two lines, but make sure that you have the latest\n",
    "# Statsmodels master from Github.  This is only needed for the\n",
    "# RegressionFDR analysis at the end of the notebook.\u0013\n",
    "import sys\n",
    "sys.path.insert(0, \"/afs/umich.edu/user/k/s/kshedden/statsmodels_fork/statsmodels\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "from scipy.stats.distributions import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very simplistically, suppose we conduct five independent tests and\n",
    "report a positive finding if at least one of them looks interesting.  Here, we\n",
    "work with Z-scores, noting that a Z-score that is bigger than two in\n",
    "magnitude is equivalent to a p-value being smaller than 0.05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The simulation sample size, any large number will do.\n",
    "nrep = 10000\n",
    "\n",
    "z = np.random.normal(size=(nrep, 5))\n",
    "print((np.abs(z).max(1) > 2).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain a value of around 20%, meaning that we have a 20% change of\n",
    "rejecting at least one null hypothesis falsely, even though the\n",
    "probability of rejecting each individual null hypothesis falsely is\n",
    "\n",
    "5%.  Note that in this simple case where the hypotheses are\n",
    "independent, we did not need to use simulation to calculate this\n",
    "quantity, it is exactly equal to $1 - (1 - 0.05)^5$.\n",
    "\n",
    "Next we have a slightly more elaborate example.  We have an outcome\n",
    "$y$ and a variable of interest $x$, along with five potential\n",
    "confounders.  We regress $y$ on $x$ and one of the confounders,\n",
    "yielding five regressions.  If $y$ and $x$ are statistically\n",
    "associated in any of these five models, we claim that an association\n",
    "has been found.  In the example below, the outcome is independent of\n",
    "all covariates.  Thus, all rejections of the null hypothesis are false\n",
    "positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of simulation replications, any large value will do\n",
    "nrep = 500\n",
    "\n",
    "# Sample size\n",
    "n = 100\n",
    "\n",
    "# Correlation between each confounder and the exposure\n",
    "r = 0.4\n",
    "\n",
    "reject = 0\n",
    "for i in range(nrep):\n",
    "\n",
    "    x = np.random.normal(size=(n, 6))\n",
    "\n",
    "    # The outcome is independent of all predictors\n",
    "    y = np.random.normal(size=n)\n",
    "\n",
    "    # Check if we reject the null of no treatment effect\n",
    "    # for at least one choice of confounder\n",
    "    reject1 = 0\n",
    "    for j in range(1, 6):\n",
    "        x[:, j] = r*x[:, 0] + np.sqrt(1 - r**2)*x[:, j]\n",
    "        zscore = sm.OLS(y, x[:, [0, j]]).fit().tvalues\n",
    "        if np.abs(zscore[0]) > 2:\n",
    "            reject1 += 1\n",
    "    reject += int(reject1 > 0)\n",
    "\n",
    "# The false positive rate of this procedure\n",
    "print(reject / nrep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the simulation above, we see that the FWER is around double\n",
    "its \"nominal\" value of 0.05.\n",
    "\n",
    "# Simple ways to address multiple testing\n",
    "\n",
    "A very simple approach for addressing multiple testing is to multiply\n",
    "all p-values by the number of tests that were performed.  This is the\n",
    "\"Bonferroni adjustment\".  Equivalently, we can define a test statistic\n",
    "threshold based on a per-test type-I error rate of $\\alpha/m$, where\n",
    "$\\alpha$ (e.g. 0.05) is the desired FWER, and $m$ is the number of\n",
    "tests.\n",
    "\n",
    "The Bonferroni adjustment can be moderately to extremely conservative\n",
    "if the tests being conducted are dependent with each other.  If the\n",
    "tests are independent, the Bonferroni procedure is not conservative to\n",
    "a meaningful degree.  By \"dependent\" here, we mean that two tests are\n",
    "dependent if the test statistics used to decide the results of the\n",
    "tests are non-independent random variables.\n",
    "\n",
    "The performance of the Bonferroni adjustment is illustrated in the\n",
    "following simulation study.  We use correlation coefficients here as\n",
    "an example.  All population correlation coefficients are zero, so all\n",
    "rejected null hypotheses are false positives.  Our hypothesis tests\n",
    "are conducted by applying a variance-stabilizing transformation to the\n",
    "Pearson correlation coefficient (the \"Fisher transformation\").  The\n",
    "results of this simulation show that when the tests are independent (r\n",
    "= 0), the Bonferroni approach is tight (around 5% of null hypotheses\n",
    "are rejected).  But when the tests are dependent (r > 0), the\n",
    "Bonferroni approach becomes conservative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of tests\n",
    "m = 100\n",
    "\n",
    "# Sample size\n",
    "n = 100\n",
    "\n",
    "# Monte Carlo replications (any large number will do)\n",
    "nrep = 1000\n",
    "\n",
    "# Generate data that is AR(r) within each column\n",
    "def genar(m, n, r):\n",
    "\tx = np.random.normal(size=(m, n))\n",
    "\tfor i in range(1, n):\n",
    "\t\tx[i, :] = r*x[i-1, :] + np.sqrt(1 - r**2)*x[i, :]\n",
    "\treturn x\n",
    "\n",
    "for r in 0, 0.99:\n",
    "    reject = 0\n",
    "    for k in range(nrep):\n",
    "\n",
    "        x1 = genar(m, n, r)\n",
    "        x2 = genar(m, n, r)\n",
    "\n",
    "        c = [np.corrcoef(x1[i, :], x2[i, :])[0, 1] for i in range(n)]\n",
    "        c = np.asarray(c)\n",
    "\n",
    "        # Apply a variance-stabilizing transformation to the correlation\n",
    "        # estimates\n",
    "        f = 0.5 * np.log((1 + c) / (1 - c))\n",
    "\n",
    "        # Z-scores\n",
    "        z = f * np.sqrt(n - 3)\n",
    "\n",
    "        # p-values\n",
    "        pv = 2 * norm.cdf(-np.abs(z))\n",
    "\n",
    "        # Bonferroni adjusted p-values\n",
    "        bpv = m * pv\n",
    "\n",
    "        reject += np.any(bpv < 0.05)\n",
    "\n",
    "    print(reject / nrep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# False Discovery Rates\n",
    "\n",
    "The Bonferroni approach aims to control the type-I error rate, which\n",
    "is the probability of making at least one false statement among all\n",
    "claims made in a study.  If the cost of a false positive is high (e.g.\n",
    "the consequences are great, and it is expensive or difficult to\n",
    "conduct additional studies on independent data to further validate the\n",
    "finding), this is arguably the proper approach to take.  But in other\n",
    "research settings, it is relatively cheap to follow up all the initial\n",
    "positive findings from a study with additional validations.  In this\n",
    "type of work, it seems too conservative to control the probability of\n",
    "even one false claim being made, especially since this reduces our\n",
    "power for making discoveries.\n",
    "\n",
    "The False Discovery Rate (FDR) is the proportion of all claimed\n",
    "positives that are false positives.  Controlling the FDR is arguably\n",
    "more appropriate than controlling the FWER in settings where it is\n",
    "relatively easy to validate findings.  Here are two concrete examples\n",
    "where people may argue that controlling the FDR is more appropriate\n",
    "than controlling the FWER:\n",
    "\n",
    "* Suppose we are screening financial transactions for possible fraud.\n",
    "Each day, millions of transactions are screened, and a small fraction\n",
    "of them look suspicious.  The suspicious transactions are then\n",
    "manually checked.  Under FWER control, we aim to limit the probability\n",
    "that even one transaction is deemed suspicious when it is in fact\n",
    "legitimate.  If we think of every day as being a replicate of this\n",
    "approach, and we control FWER at 5%, then on 19 out of 20 days, no\n",
    "false positives will occur.  On the other hand, if we control the FDR\n",
    "at 5%, this means that of all transactions flagged as suspicious, 95%\n",
    "of them (on average) turn out to be fraudulent.  That is, 95% of the\n",
    "effort spent assessing suspicious transactions leads to the conclusion\n",
    "that the transaction is indeed problematic.\n",
    "\n",
    "* Suppose we are screening drug candidates for a disease using an \"in\n",
    "vitro\" assay that can be carried out rapidly and cheaply by robots.\n",
    "There are millions of drug candidates.  A candidate that looks\n",
    "possibly \"active\" will be assessed further with additional assays.  It\n",
    "is expected that most of the positives from the first round will turn\n",
    "out to be false positives, but if even one succeeds this is a major\n",
    "victory.  If we control the FDR, even at a slack value such as 90%,\n",
    "then we are operating in a way that is consistent with the project\n",
    "goals.\n",
    "\n",
    "Note that while FWER is indeed superior than FDR at limiting false\n",
    "positives, it achieves this by using a stricter threshold to determine\n",
    "what is a true positive.  This means that FWER will generally have\n",
    "lower power than FDR.\n",
    "\n",
    "There are a number of ways to calculate the FDR.  Here we focus on two\n",
    "of them.  One, is a simplified version of the \"Benjamini-Hochberg\"\n",
    "FDR, which uses simple empirical proportions that directly mimic the\n",
    "population version of the FDR statistic.  The other is a \"local\"\n",
    "version of the FDR devised by Efron.\n",
    "\n",
    "## Global FDR\n",
    "\n",
    "Suppose we have null hypotheses $H_1, H_2, \\ldots, H_m$.  Define\n",
    "$N_i=1$ if null hypothesis $i$ is true, and $N_i=0$ otherwise.  Define\n",
    "$R_i=1$ if null hypothesis $i$ is rejected at a particular evidence\n",
    "threshold $T$, and $R_i=0$ otherwise.  Then the FDR can be defined as\n",
    "\n",
    "$$\n",
    "\\frac{\\sum_i R_i\\cdot N_i}{\\sum_i R_i} =\n",
    "\\frac{m^{-1}\\sum_i R_i\\cdot N_i}{m^{-1}\\sum_i R_i}.\n",
    "$$\n",
    "\n",
    "A simple way to estimate this quantity is to assume (for the sake of\n",
    "calculation) that all the hypotheses are null, i.e., that $N_i = 0$\n",
    "for all $i$.  This generally leads to only a small amount of bias,\n",
    "since most of the null hypotheses actually are true in most practical\n",
    "settings.  In this case, the numerator $m^{-1}\\sum_i R_i\\cdot N_i$ can\n",
    "be estimated by the number of rejected tests, and the denominator\n",
    "$m^{-1} \\sum_i R_i$ is approximately the type-I error rate of the\n",
    "individual tests when conducted at threshold $T$.\n",
    "\n",
    "Thus, for a given testing threshold $T$, we can define the type-I\n",
    "error rate of the procedure.  There are various ways to obtain the FDR\n",
    "for a specific test.  One approach is to take the test statistic $T_i$\n",
    "for test $i$ (so that $R_i = {\\cal I}(T_i >T)$), and calculate the FDR\n",
    "for $T = T_i$.  The FDR for this procedure can be used to define an\n",
    "FDR for test $i$.  There are alternative procedures known as \"step-up\"\n",
    "procedures that are often used in practice to obtain the FDR for a\n",
    "specific hypothesis.  But this approach gives similar results and is\n",
    "more intuitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(194)\n",
    "\n",
    "# Number of tests\n",
    "m = 10000\n",
    "\n",
    "# Sample size per group\n",
    "n = 50\n",
    "\n",
    "# Number of true alternatives\n",
    "q = 50\n",
    "\n",
    "x1 = np.random.normal(size=(m, n))\n",
    "x2 = np.random.normal(size=(m, n))\n",
    "\n",
    "# The first q tests are true alternatives, the others\n",
    "# are true nulls\n",
    "x1[0:q, :] += 0.5\n",
    "\n",
    "# Z-scores\n",
    "se = np.sqrt(x1.var(1)/n + x2.var(1)/n)\n",
    "z = (x1.mean(1) - x2.mean(1)) / se\n",
    "za = np.abs(z)\n",
    "\n",
    "# The Z-score for the first test (which is a true alternative)\n",
    "print(za[0])\n",
    "\n",
    "# The number of tests that are at least as strong as the first test\n",
    "# in terms of evidence against the null\n",
    "print(np.sum(za >= za[0]))\n",
    "\n",
    "# The expected number of tests that would be as strongly against\n",
    "# the null as the first test, if all null hypotheses were true\n",
    "print(m * norm.cdf(-za[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results above, a rough estimate of the FDR at threshold\n",
    "$T=2.74$ is $30.6/86 \\approx 0.36$.  This is also the FDR for the\n",
    "first test.  The FDR values for the other tests can be calculated\n",
    "similarly.  This approach to estimating the FDR has been called the\n",
    "\"Bayesian FDR\".\n",
    "\n",
    "We see that in this setting, a Z-score of 2.74 gives an FDR value of\n",
    "0.36.  Below we calculate the single-test p-value, and the Bonferroni\n",
    "adjusted p-value for this Z-score.  The single test p-value is very\n",
    "small, but due to the effect of multiple testing, it's not clear what\n",
    "this means in terms of overall evidence against the null hypothesis.\n",
    "The Bonferroni-corrected p-value is greater than 1.  It would usually\n",
    "be reported as 1, indicating that the FWER is close to 100%.\n",
    "\n",
    "The Z-score of 4.566, also calculated below, is the minimum Z-score\n",
    "magnitude that would achieve a Bonferroni adjusted p-value of 0.05\n",
    "when $m=10,000$ tests are performed.  As we can see, $Z=2.74$ is far\n",
    "below the needed value of 4.56.  Thus we see than when 10,000 tests\n",
    "are performed, $Z=2.74$ is not sufficient evidence to be confident\n",
    "that a null hypothesis does not hold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The p-value for an observed Z-score of 2.74.\n",
    "p = 2*norm.cdf(-2.74)\n",
    "print(p)\n",
    "\n",
    "# The Bonferroni adjusted p-value\n",
    "print(m*p)\n",
    "\n",
    "# Minimum Z-score magnitude needed to achieve Bonferroni\n",
    "# adjusted p-value less than 0.05.\n",
    "print(-norm.ppf(0.025/m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statsmodels provides several different ways to calculate FDR values,\n",
    "but the \"Bayesian\" approach discussed above is not one of them.  In\n",
    "the next cell, we calculate the \"Benjamini Hochberg\" FDR for the same\n",
    "simulated data considered above.  We see that of the 50 \"true\n",
    "alternatives\", 6 have FDR < 0.1, and would likely be considered as\n",
    "\"discoveries\" in practice.  None of the \"true nulls\" has FDR < 0.1, so\n",
    "in this case, we achieve an FDR of 0, with 6 discoveries.  The\n",
    "achieved FDR is random, and on average, the true FDR of a test is no\n",
    "greater than its estimated FDR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdr0 = norm.cdf(-za[0]) / np.mean(za >= za[0])\n",
    "print(fdr0)\n",
    "\n",
    "pv = 2*norm.cdf(-za)\n",
    "_, gfdr, _, _ = sm.stats.multipletests(pv, method=\"fdr_bh\")\n",
    "print(np.sum(gfdr[0:q] < 0.1))\n",
    "print(np.sum(gfdr[q:] < 0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't cover this further here, but in practice it is important to consider\n",
    "whether the Z-score (or p-values) being considered in an FDR analysis are\n",
    "statistically independent, in the sense discussed above.  The basic approaches\n",
    "to FDR control discussed above are robust to a certain amount of dependence.\n",
    "There are some alternative approaches to estimating FDR values that handle\n",
    "stronger dependence of certain forms, but there isn't a practical way at\n",
    "present to estimate FDR values in a way that handles arbitrary patterns of\n",
    "dependence.\n",
    "\n",
    "# Local FDR\n",
    "\n",
    "An alternative approach to FDR known as \"local FDR” has been advocated\n",
    "by Efron. At a high level, the distinction between FDR and local FDR\n",
    "is that local FDR is based on densities while FDR is based on tail\n",
    "probabilities.  In global FDR, if we have an evidence threshold $T$\n",
    "(i.e.  something that we compare a Z-score to), then we compare the\n",
    "number of tests with $Z > T$ to the expected number of such tests.  In\n",
    "local FDR, we compare the number of tests with $Z \\approx T$ to the\n",
    "expected number of such tests.\n",
    "\n",
    "Local and global FDR can both be defined in terms of Z-scores. The\n",
    "local FDR at a particular Z-score value is defined as the ratio of two\n",
    "densities evaluated at Z, $f(Z)/g(Z)$. The numerator density $f$ is\n",
    "the density of null Z-scores, and the denominator density $g$ is the\n",
    "density of all Z-scores, which is presumed to be a mixture of null and\n",
    "non-null Z-scores.\n",
    "\n",
    "If the local FDR takes on a value, say 0.1 at $Z\\approx 2.5$, this\n",
    "means that the actual distribution of Z-scores generates values around\n",
    "2.5 at 10 times the rate that the reference distribution generates\n",
    "such values.\n",
    "\n",
    "In most cases, $f$ is simply a standard normal density, since most\n",
    "test statistics can be taken to follow a standard normal distribution\n",
    "when the null hypothesis is true. The denominator density $g$ could be\n",
    "estimated with a simple histogram method, but it is most commonly\n",
    "estimated using Poisson regression, following an approach known as\n",
    "\"Lindley’s method\". The density $g$ is modeled as\n",
    "\n",
    "$$\n",
    "g(z) = \\exp(\\sum_j \\beta_j z^j)\n",
    "$$\n",
    "\n",
    "In Lindley's method, the parameters $\\beta$ are estimated using\n",
    "Poisson regression. The observed range of Z-scores is partitioned into\n",
    "bins, and we count the number of Z-scores that fall into each\n",
    "bin. These counts are regressed against the bin centers (and a\n",
    "polynomial basis of these values) using Poisson regression. This is\n",
    "essentially a way of smoothing a histogram, using Poisson regression to\n",
    "do the smoothing.\n",
    "\n",
    "The cell below illustrates how to calculate local FDR values using statsmodels.\n",
    "In this case, local FDR discovered only 4 of the \"true alternatives\", and like\n",
    "the global FDR achieved an FDR of 0.  In general, either the local or\n",
    "global FDR can be more powerful, depending on the setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfdr = sm.stats.local_fdr(z)\n",
    "\n",
    "print(np.sum(lfdr[0:q] < 0.1))\n",
    "print(np.sum(lfdr[q:] < 0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression FDR and the knockoff filter\n",
    "\n",
    "The approaches to FDR discussed above are suited for “marginal\n",
    "screening”. The generally means that distinct variables are considered\n",
    "in each test.  A different, but related question arises when fitting\n",
    "regression models with large numbers of covariates. Here, we are faced\n",
    "with the question of variable selection, for a collection of $p$\n",
    "covariates $x_1, \\ldots, x_p$ that may predict an outcome $y$.  In\n",
    "regression variable selection, the null hypotheses are not asking\n",
    "about the presence or absence of marginal dependence, e.g. through\n",
    "${\\rm cor}(x_j, y)$. Instead, we are asking whether $x_j$ is\n",
    "independent of $y$ conditioned on $\\{x_k, k\\ne j\\}$.\n",
    "\n",
    "There are many methods for variable selection, here we focus on a\n",
    "recently-proposed approach to variable selection that utilizes the\n",
    "idea of FDR. The FDR approaches discussed above would not directly\n",
    "address this question, since they look at marginal not conditional\n",
    "relationships. One possible resolution is the “knockoff filter”.\n",
    "\n",
    "The basic idea of the knockoff filter is that we augment our covariate\n",
    "set with a collection of \"knockoffs\", doubling the number of\n",
    "covariates in the model. The knockoff variables are in one-to-one\n",
    "correspondence with the actual variables, so we can write\n",
    "$\\tilde{x}_j$ as the knockoff counterpart to $x_j$. The knockoff\n",
    "variables need to be constructed in a very particular way. First, two\n",
    "knockoff variables must be correlated with each other in the same way\n",
    "that their non-knockoff counterparts are correlated. That is, ${\\rm\n",
    "cov}(\\tilde{x}_j, \\tilde{x}_k) = {\\rm cov}(x_j, x_k)$.  In addition,\n",
    "the knockoff variables need to be coupled to their non-knockoff\n",
    "counterparts: ${\\rm cov}(x_j, \\tilde{x}_k) = {\\rm cov}(x_j, x_k)$,\n",
    "where $j\\ne k$, and ${\\rm cov}(x_j, \\tilde{x}_j) = 1−s_j$, where $s_j$\n",
    "is a tuning parameter which we will not discuss in detail here.\n",
    "\n",
    "The knockoff filter works by regression $y$ on all the variables (both\n",
    "the actual variables and their knockoff counterparts). We then define\n",
    "a statistic that measures whether a given actual variable has a\n",
    "stronger role in the model than its knockoff counterpart. A basic\n",
    "choice for such a statistic would be $T_j \\equiv |\\beta_j| −\n",
    "|\\beta_{j+p}|$.  Next, we order the variables based on decreasing\n",
    "values of $T_j$, placing the variables with greatest $T_j$ at the\n",
    "beginning of the list. Finally a \"stepdown\" procedure is used to\n",
    "assign an FDR to each variable in this list. We will not discuss the\n",
    "details of the stepdown procedure here.\n",
    "\n",
    "The knockoff procedure can be seen to accurately control the FDR under\n",
    "fairly weak conditions. Note in particular that we do not need to do\n",
    "any detailed theoretical analysis of the particular modeling procedure\n",
    "being used, which would be needed for most classical\n",
    "approaches to inference. For this reason, the knockoff filter can\n",
    "be applied to many modern modeling methods like the Lasso\n",
    "which are difficult to approach in a rigorous inferential manner using\n",
    "other techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats import knockoff_regeffects as kr\n",
    "\n",
    "n = 500\n",
    "p = 20\n",
    "\n",
    "# Generate covariates that are somewhat dependent\n",
    "x = np.random.normal(size=(n, p))\n",
    "r = 0.2\n",
    "for j in range(1, p):\n",
    "    x[:, j] = r*x[:, j-1] + np.sqrt(1-r**2)*x[:, j]\n",
    "\n",
    "# Generate the dependent variable of the regression\n",
    "b = np.zeros(p)\n",
    "b[0:6] = [1, 0, -1, 0, 1, 0]\n",
    "ey = np.dot(x, b)\n",
    "y = ey + np.random.normal(size=n)\n",
    "\n",
    "# This is one of several \"effect testers\" that we can\n",
    "# choose.  This one works well for regression models\n",
    "# with non-orthogonal designs.\n",
    "tester = kr.OLSEffects()\n",
    "\n",
    "kn = sm.stats.RegressionFDR(y, x, tester, \"equi\")\n",
    "print(kn.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 1
}
