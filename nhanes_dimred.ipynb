{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical dimension reduction using Python and Statsmodels -- a case study using the NHANES data\n",
    "\n",
    "This notebook illustrates several approaches to statistical dimension\n",
    "reduction, focusing on the practical aspects of performing dimension\n",
    "reduction in Python using the\n",
    "[Statsmodels](http://www.statsmodels.org) library. We will also be\n",
    "using the [Pandas](http://pandas.pydata.org) library for data\n",
    "management, and the [Numpy](http://www.numpy.org) library for\n",
    "numerical calculations.\n",
    "\n",
    "Dimension reduction encompasses a number of techniques that have the\n",
    "overarching goal of taking a collection of related variables and\n",
    "converting them to a smaller number of summary variables that contain\n",
    "most of their information.  Many statistical techniques can be viewed\n",
    "as having this goal, including classical techniques such as Principal\n",
    "Components Analysis and more modern techniques such as autoencoders.\n",
    "\n",
    "To begin, we import several libraries that we will use below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an initial illustration, we will use the body measures (BMX) and\n",
    "blood pressure (BPX) data files from the 2015 wave of NHANES.  The\n",
    "data files can be downloaded in SAS xport format from [this\n",
    "link](https://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Examination&CycleBeginYear=2015).\n",
    "Place the two data files (BPX_I.XPT and BMX_I.XPT) in your Jupyter\n",
    "working directory to make them accessible to the code below.\n",
    "\n",
    "We will start by working with six blood pressure variables -- three\n",
    "repeated measures of systolic blood pressure and three repeated\n",
    "measures of diastolic blood pressure.  These measures are strongly\n",
    "correlated with each other, particularly the three mesures with the\n",
    "same blood pressure type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sas(\"BPX_I.XPT\")\n",
    "\n",
    "bpx_vars = [\"BPXSY1\", \"BPXSY2\", \"BPXSY3\", \"BPXDI1\", \"BPXDI2\", \"BPXDI3\"]\n",
    "da = df.loc[:, bpx_vars + [\"SEQN\", \"BPXPLS\"]].dropna()\n",
    "print(da.shape)\n",
    "print(da.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear dimension reduction\n",
    "\n",
    "The goal of linear dimension reduction is to define \"variates\", or\n",
    "linear combinations of the input variables, that capture some\n",
    "interesting property of the input variables.  A variate is defined\n",
    "through its coefficients, e.g. the linear combination\n",
    "\n",
    "$$\n",
    "b_1\\cdot{\\rm BPXSY1} + b_2\\cdot{\\rm BPXSY2} + b_3\\cdot{\\rm BPXSY3} + b_4\\cdot{\\rm BPXDI1} + b_5\\cdot{\\rm BPXDI2} + b_6\\cdot{\\rm BPXDI3}\n",
    "$$\n",
    "\n",
    "is a variate, defined by coefficients (loadings) $b_1, \\ldots, b_6$.\n",
    "\n",
    "Different approaches to linear dimension reduction use different\n",
    "methods for defining the variate loadings.  For example, Principal\n",
    "Components Analysis (PCA) constructs variates that capture the\n",
    "greatest fraction of the information in the input data.\n",
    "\n",
    "As noted above, the \"loadings\" are the coefficients that define how\n",
    "each input variable relates to the reduced variables.  The \"scores\"\n",
    "are the numbers that are obtained by taking the data for one unit of\n",
    "analysis and combining its data using the loadings.  If there are $p$\n",
    "input variables and $n$ observations, and we use a linear dimension\n",
    "reduction technique to reduce to $q$ variables, then the loadings\n",
    "matrix has shape $p \\times q$, and the scores matrix has dimension\n",
    "$n\\times q$.\n",
    "\n",
    "## Covariance and correlation matrices\n",
    "\n",
    "Classical dimension reduction techniques often start by considering\n",
    "the covariance matrix of the variables of interest.  This matrix\n",
    "contains a lot of information about the pairwise relationships among\n",
    "the variables.  Recall that if we have three variables, X, Y, and Z,\n",
    "then the covariance matrix among these variables is a $3\\times 3$\n",
    "matrix with the following entries:\n",
    "\n",
    "$$\n",
    "\\left(\\begin{array}{ccc}\n",
    "{\\rm Var}(X)     &{\\rm Cov}(X, Y) & {\\rm Cov}(X, Z)\\\\\n",
    "{\\rm Cov}(Y, X)  &{\\rm Var}(Y)    & {\\rm Cov}(Y, Z)\\\\\n",
    "{\\rm Cov}(Z, X)  &{\\rm Cov}(Z, Y) & {\\rm Var}(Z)\\\\\n",
    "\\end{array}\\right).\n",
    "$$\n",
    "\n",
    "If we standardize the data, then we obtain the correlation matrix\n",
    "which describes the pair-wise relationships among the variables in a\n",
    "unit-free way.\n",
    "\n",
    "$$\n",
    "\\left(\\begin{array}{ccc}\n",
    "1     &{\\rm Cor}(X, Y) & {\\rm Cor}(X, Z)\\\\\n",
    "{\\rm Cor}(Y, X)  &1    & {\\rm Cor}(Y, Z)\\\\\n",
    "{\\rm Cor}(Z, X)  &{\\rm Cor}(Z, Y) & 1\\\\\n",
    "\\end{array}\\right).\n",
    "$$\n",
    "\n",
    "In the next cell, we calculate the $6\\times 6$ correlation matrix of\n",
    "the blood pressure measures and plot it as a heatmap.  We can see that\n",
    "there are very strong correlations between two systolic blood pressure\n",
    "measures, and strong (but slightly weaker) correlations between two\n",
    "diastolic blood pressure measures.  A systolic and a diastolic measure\n",
    "are also positively related, but with a correlation coefficient of\n",
    "around 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpx_cor = np.corrcoef(da.loc[:, bpx_vars].T)\n",
    "plt.imshow(bpx_cor, vmin=0, vmax=1)\n",
    "_ = plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The spectral decomposition\n",
    "\n",
    "Many dimension reduction techniques make heavy use of an important\n",
    "technique from linear algebra known as the \"spectral decomposition\",\n",
    "or \"eigen-decomposition\".  The basic idea is to take a symmetric,\n",
    "positive definite matrix $C$ (i.e. a covariance matrix), and decompose\n",
    "it in the form $C = QDQ^\\prime$, where $Q$ is an orthogonal matrix\n",
    "with the same shape as $C$ (so $Q^\\prime Q = I$), and $D$ is a\n",
    "diagonal matrix with decreasing non-negative entries along the main\n",
    "diagonal.\n",
    "\n",
    "In the next code cell, we calculate the eigen-decomposition of the\n",
    "covariance matrix among the six blood pressure measures, and verify\n",
    "that it satisfies the expected mathematical properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eiv, eig = np.linalg.eig(bpx_cor)\n",
    "\n",
    "# Verify that bpx_cor = eig * eiv * eig'\n",
    "di = bpx_cor - np.dot(eig, np.dot(np.diag(eiv), eig.T))\n",
    "print(np.max(np.abs(di)))\n",
    "\n",
    "# Verify that eig is orthogonal\n",
    "di = np.dot(eig, eig.T)\n",
    "print(np.max(np.abs(di - np.eye(6))))\n",
    "\n",
    "# The eigenvalues are not sorted, so we sort them\n",
    "ii = np.argsort(eiv)[::-1]\n",
    "eiv = eiv[ii]\n",
    "eig = eig[:, ii]\n",
    "\n",
    "# The eigenvalues of a covariance matrix must be non-negative\n",
    "print(eiv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal components analysis\n",
    "\n",
    "The columns of $Q$ define linear combinations of the input variables\n",
    "that capture the greatest possible fraction of their variation.  The\n",
    "first column of $Q$ is the \"dominant principal component\" and defines\n",
    "the single best linear combination for preserving the information in\n",
    "the data.  After this component is accounted for, the second column of\n",
    "$Q$ (the \"second principal component\") defines the most informative\n",
    "variate that is uncorrelated with the dominant variate.  The\n",
    "subsequent principal components are constructed similarly.\n",
    "\n",
    "PCA can be viewed from several different perspectives.  One important\n",
    "perspective is that we are \"encoding\" the data to a lower dimension in\n",
    "such a way that the greatest possible fraction of information in the\n",
    "input variables is retained in the reduced variables.  If $P$ is a\n",
    "$p\\times q$ projection matrix that carries out the dimension\n",
    "reduction, then the optimal $P$ matrix for PCA minimizes $E[\\|y -\n",
    "Py\\|^2]$.\n",
    "\n",
    "As a special case, for the first variate (dominant principal\n",
    "component), this optimization reduces to minimizing $E[(y - \\langle b,\n",
    "y \\rangle\\cdot b)^2]$ over all unit vectors $b$.\n",
    "\n",
    "For the NHANES blood pressure data, the dominant three principal\n",
    "components are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eig[:, 0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that principal components are only defined up to sign, so it\n",
    "would be equally correct to say that the principal components are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(-eig[:, 0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PC's can be interpreted as follows.  The dominant PC is\n",
    "essentially an equally-weighted positive combination of all six blood\n",
    "pressure measures.  The systolic measure receives a slightly higher\n",
    "weight in the PC than the diastolic measure, which is likely beacuse\n",
    "the diastolic measures are less correlated with each other.  The\n",
    "second PC reflects the difference between the two types of blood\n",
    "pressure. The third PC reflects an individual-specific tendency for\n",
    "the three repeated measures to rise of fall together, with much\n",
    "greater amplitude for the trend in the diastolic values.\n",
    "\n",
    "The principal components are often considered in light of the variance\n",
    "that each component explains, as given by the calculation below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eiv**2 / np.sum(eiv**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dominant principal component explains 85% of the variance in the\n",
    "six measures, the next component explains 14%, and the remaining\n",
    "components contribute very little.\n",
    "\n",
    "### The PC scores\n",
    "\n",
    "The PC loadings describe the variables, to describe the observations\n",
    "we use the scores.  Since above we carried out our PCA using\n",
    "standardized data, we first standardize the blood pressure data, then\n",
    "calculate the scores using the loading vectors obtained from the\n",
    "spectral decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = da.loc[:, bpx_vars]\n",
    "ds -= ds.mean(0)\n",
    "ds /= da.std(0)\n",
    "\n",
    "scores = np.dot(ds, eig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By construction, these scores are uncorrelated and have mean zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scores.mean(0))\n",
    "print(np.around(100*np.cov(scores.T)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is common to plot scores for different components against each\n",
    "other in a scatterplot.  By construction, this scatterplot is centered\n",
    "around the origin, and the two coordinates are uncorrelated.  However\n",
    "the data need not be perfectly elliptically-distributed.  For example,\n",
    "here, the second PC score distribution is substantially skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.grid(True)\n",
    "plt.plot(scores[:, 0], scores[:, 1], 'o', alpha=0.4)\n",
    "plt.xlabel(\"PC 1 scores\")\n",
    "_ = plt.ylabel(\"PC 2 scores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to understand what the PC reduction means is to plot the PC\n",
    "scores against an auxiliary variable that was not part of the PC\n",
    "reduction.  Here we will use \"BPXPLS\", which is a subject's pulse\n",
    "rate.\n",
    "\n",
    "We see that there is a weak tendency for people with higher pulse rate\n",
    "tend to have greater PC1 scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "yx = lowess(scores[:,0], da.loc[:, \"BPXPLS\"], frac=0.3)\n",
    "plt.grid(True)\n",
    "plt.plot(da.loc[:, \"BPXPLS\"], scores[:, 0], 'o', alpha=0.4)\n",
    "plt.plot(yx[:, 0], yx[:, 1], lw=4, color='orange')\n",
    "plt.xlabel(\"Pulse rate\")\n",
    "plt.ylabel(\"PC1 score\")\n",
    "\n",
    "print(np.corrcoef(da.loc[:, \"BPXPLS\"], scores[:, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There isn't much of a relationship between PC2 and pulse rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yx = lowess(scores[:,1], da.loc[:, \"BPXPLS\"], frac=0.3)\n",
    "plt.grid(True)\n",
    "plt.plot(da.loc[:, \"BPXPLS\"], scores[:, 1], 'o', alpha=0.4)\n",
    "plt.plot(yx[:, 0], yx[:, 1], lw=4, color='orange')\n",
    "plt.xlabel(\"Pulse rate\")\n",
    "plt.ylabel(\"PC2 score\")\n",
    "\n",
    "print(np.corrcoef(da.loc[:, \"BPXPLS\"], scores[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach to understanding how this two-dimensional data\n",
    "reduction relates to the original data, we can select points in\n",
    "different regions of the score scatterplot, and then plot their data\n",
    "values.  Note that the data values plotted below are centered and\n",
    "standardized, so they are scaled deviations from the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.grid(True)\n",
    "plt.plot(scores[:, 0], scores[:, 1], 'o', alpha=0.4)\n",
    "plt.xlabel(\"PC 1 scores\")\n",
    "plt.ylabel(\"PC 2 scores\")\n",
    "\n",
    "i0 = (scores[:, 0] < -4) & (np.abs(scores[:, 1]) < 1)\n",
    "plt.plot(scores[i0, 0], scores[i0, 1], 'o', color='purple')\n",
    "\n",
    "i1 = (scores[:, 0] > 4) & (np.abs(scores[:, 1]) < 1)\n",
    "plt.plot(scores[i1, 0], scores[i1, 1], 'o', color='orange')\n",
    "\n",
    "i2 = (scores[:, 1] > 3) & (np.abs(scores[:, 0]) < 1)\n",
    "plt.plot(scores[i2, 0], scores[i2, 1], 'o', color='lime')\n",
    "\n",
    "i3 = (scores[:, 1] < -1.5) & (np.abs(scores[:, 0]) < 1)\n",
    "plt.plot(scores[i3, 0], scores[i3, 1], 'o', color='yellow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the dominant PC axis differentiates people with higher\n",
    "than average systolic and diastolic blood pressure (left, purple),\n",
    "from people with lower than average systolic and diastolic blood\n",
    "pressure (right, orange).  The second PC identifies people who have\n",
    "higher than average systolic blood pressure but lower than average\n",
    "diastolic blood pressure (top, green), from people who have higher\n",
    "than average diastolic blood pressure and lower than average systolic\n",
    "blood pressure (bottom, yellow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.grid(True)\n",
    "\n",
    "for i in np.flatnonzero(i0):\n",
    "    plt.plot(ds.iloc[i, :], color='purple')\n",
    "\n",
    "for i in np.flatnonzero(i1):\n",
    "    plt.plot(ds.iloc[i, :], color='orange')\n",
    "\n",
    "for i in np.flatnonzero(i2):\n",
    "    plt.plot(ds.iloc[i, :], color='lime')\n",
    "\n",
    "for i in np.flatnonzero(i3):\n",
    "    plt.plot(ds.iloc[i, :], color='yellow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Role of marginal means and variances in PCA\n",
    "\n",
    "PCA is usually conducted using the correlation matrix, as above, but\n",
    "sometimes the covariance matrix is used instead.  In both cases, the\n",
    "mean is removed before calculating the correlations or covariances.\n",
    "While the mean and marginal variances are mostly irrelevant for\n",
    "dimension reduction, to get a complete picture of the data it is a\n",
    "good idea to briefly inspect them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(da.mean(0))\n",
    "print(da.std(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA in Statsmodels\n",
    "\n",
    "It's not difficult to conduct a PCA manually using Numpy.  But many\n",
    "people will prefer to use the higher-level PCA function from\n",
    "Statsmodels.  Below we illustrate how identical results to what we\n",
    "found above can be obtained in this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rslt = sm.PCA(da.loc[:, bpx_vars], ncomp=3)\n",
    "\n",
    "# coeff gives the loadings\n",
    "print(rslt.coeff.iloc[0:3, :].T)\n",
    "\n",
    "print(rslt.eigenvals)\n",
    "print(rslt.eigenvals**2 / np.sum(rslt.eigenvals**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noted above, the principal components are only meaningful up to a\n",
    "scaling constant, i.e. if $b$ is the dominant PC, then $k\\cdot b$ (for\n",
    "real, nonzero $k$) can also be considered to be the dominant PC.\n",
    "There are different conventions about how to scale the PC loading\n",
    "vectors.  If we scale the loadings obtained from the PCA function to\n",
    "have unit length, they agree exactly with the results obtained above\n",
    "using the eigen-decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = rslt.coeff.iloc[0:3, :].T\n",
    "c = c / np.sqrt((c**2).sum(0))\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we obtain the scores from the fitted Statsmodels PCA results\n",
    "object, and confirm that they are identical (up to scaling) with the\n",
    "scores that we obtained above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_sm = rslt.scores\n",
    "\n",
    "sc = np.hstack((scores_sm.iloc[:, 0:3], scores[:, 0:3]))\n",
    "print(np.around(100*np.corrcoef(sc.T)))\n",
    "\n",
    "plt.grid(True)\n",
    "plt.plot(scores_sm.iloc[:, 0], scores_sm.iloc[:, 1], 'o', color='purple', alpha=0.4)\n",
    "plt.xlabel(\"PC 1\")\n",
    "plt.ylabel(\"PC 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA case study: NHANES body dimensions\n",
    "\n",
    "Next we will set up another example in which PCA can be used\n",
    "productively.  We provide code for some of the initial steps, and\n",
    "leave the remainder as an exercise.  Here we consider a set of seven\n",
    "\"anthropometric\" variables based on various body dimensions.  These\n",
    "are substantially correlated, but they are all distinct measures,\n",
    "unlike above, we do not have any repeated measures of the same value.\n",
    "Therefore these measures are somewhat less correlated than the blood\n",
    "pressure values considered above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da = pd.read_sas(\"BMX_I.XPT\")\n",
    "\n",
    "bmx_vars = [\"BMXWT\", \"BMXHT\", \"BMXBMI\", \"BMXLEG\", \"BMXARML\", \"BMXARMC\", \"BMXWAIST\"]\n",
    "\n",
    "bmx = da.loc[:, bmx_vars]\n",
    "print(pd.isnull(bmx).mean(0))\n",
    "bmx = bmx.dropna()\n",
    "print(bmx.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the $7\\times 7$ correlation matrix of the anthropometric\n",
    "variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bmx_cor = np.corrcoef(bmx.T)\n",
    "plt.imshow(bmx_cor, vmin=0, vmax=1)\n",
    "_ = plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though these are measures of seven distinct quantities, they are\n",
    "still quite strongly related -- 90% of the variance can be captured\n",
    "through one component, and most of the remaining variance is captured\n",
    "by the second component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rslt = sm.PCA(bmx, ncomp=3)\n",
    "print(rslt.eigenvals**2 / np.sum(rslt.eigenvals**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All seven of the measures here are numerically larger for bigger\n",
    "people.  Based on the loadings, the dominant principal component\n",
    "captures overall body size.  The second PC captures variation that is\n",
    "related to \"stockiness\".  A person with a greater score on PC 2 tends\n",
    "to have greater weight, BMI, arm circumference and leg circumference,\n",
    "and tends to have lesser height, leg length, and arm length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rslt.coeff.iloc[0:2, :].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factor analysis\n",
    "\n",
    "Factor-analysis is a technique for dimension reduction that uses a probability\n",
    "model to aid in understanding how well the reduced variables approximate the\n",
    "input variables.  Specifically, factor analysis introduces\n",
    "independent random noise between the factors and the data.\n",
    "\n",
    "If $y$ is the observed\n",
    "vector of $p$ variables for one case, we can write $z$ for the $q$-factor approximation\n",
    "to $y$.  The residuals $y - z$ are treated as being independent random values with\n",
    "mean zero.  Each element of $y - z$ has its own variance called the \"uniqueness\".\n",
    "Factor analysis is usually performed using mean-centered, standardized data.  Therefore\n",
    "the uniquenesses will always fall between 0 and 1.\n",
    "\n",
    "Factor analysis uses the terms \"score\" and \"loading\" in very analogous ways to the use\n",
    "of these terms in Principal Component Analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.multivariate.factor import Factor\n",
    "fa = Factor(bmx, n_factor=3, method='ml').fit()\n",
    "print(fa.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Factor analysis models may have a lot of parameters and the parameters\n",
    "may not be well-identified.  Looking at the gradient (score vector) at\n",
    "the approximated MLE may provide more insights about how close we are\n",
    "to a well-defined MLE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fa.model.score([fa.loadings, fa.uniqueness]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Canonical Correlation Analysis (CCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sas(\"BPX_I.XPT\")\n",
    "bpx_vars = [\"BPXSY1\", \"BPXSY2\", \"BPXSY3\", \"BPXDI1\", \"BPXDI2\", \"BPXDI3\"]\n",
    "bpx = df.loc[:, bpx_vars + [\"SEQN\"]].dropna()\n",
    "\n",
    "df = pd.read_sas(\"BMX_I.XPT\")\n",
    "bmx_vars = [\"BMXWT\", \"BMXHT\", \"BMXBMI\", \"BMXLEG\", \"BMXARML\", \"BMXARMC\", \"BMXWAIST\"]\n",
    "bmx = df.loc[:, bmx_vars + [\"SEQN\"]].dropna()\n",
    "\n",
    "bpmx = pd.merge(bpx, bmx, left_on=\"SEQN\", right_on=\"SEQN\")\n",
    "bpx = bpmx.loc[:, bpx_vars]\n",
    "bmx = bpmx.loc[:, bmx_vars]\n",
    "\n",
    "from statsmodels.multivariate.cancorr import CanCorr\n",
    "\n",
    "ca = CanCorr(bpx, bmx)\n",
    "\n",
    "print(ca.corr_test())\n",
    "\n",
    "print(ca.x_cancoef[:, 0:3])\n",
    "print(ca.y_cancoef[:, 0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 1
}
